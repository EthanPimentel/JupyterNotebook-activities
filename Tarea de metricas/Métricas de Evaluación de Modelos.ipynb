{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8fe7bf-3707-4978-a486-a961280436b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Cargar un conjunto de datos de ejemplo\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945df17a-5c55-4177-ae8d-0bf56088b3c2",
   "metadata": {},
   "source": [
    "## Métricas de Evaluación de Modelos\n",
    "\n",
    "Las métricas de evaluación como precisión, recall, y F1-score son esenciales para entender diferentes aspectos del rendimiento del modelo. Aquí demostramos cómo calcular estas métricas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f0065f-0b26-448d-9236-aee29dcc39f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "Recall: 0.99\n",
      "Precision: 0.96\n",
      "F1 Score: 0.97\n",
      "ROC AUC Score: 0.96\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113efba-9160-4412-b82b-15b51d353074",
   "metadata": {},
   "source": [
    "Estas métricas indican un alto desempeño del modelo en el conjunto de prueba, reflejando su capacidad para predecir correctamente tanto las clases positivas como las negativas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d89c27-37e2-4b94-9706-5b7fc07112fd",
   "metadata": {},
   "source": [
    "### Explicación de las Métricas de Evaluación y sus Resultados\n",
    "\n",
    "En el contexto de la validación y evaluación de modelos de machine learning, es fundamental entender qué mide cada métrica y cómo interpretar los resultados obtenidos. A continuación, se presentan las definiciones y el análisis de las métricas de evaluación: Accuracy, Recall, Precision, F1 Score y ROC AUC Score, junto con sus resultados específicos.\n",
    "\n",
    "#### Accuracy (Precisión)\n",
    "**Definición:**\n",
    "La precisión es la proporción de predicciones correctas realizadas por el modelo sobre el total de predicciones. Es una métrica global que indica qué tan bien está funcionando el modelo en general.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{TP + TN}}{\\text{TP + TN + FP + FN}}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- TP: Verdaderos Positivos\n",
    "- TN: Verdaderos Negativos\n",
    "- FP: Falsos Positivos\n",
    "- FN: Falsos Negativos\n",
    "\n",
    "**Resultado: 0.96**\n",
    "- **Interpretación:** El modelo predice correctamente el 96% de los casos. Esto indica un buen rendimiento general, pero no distingue entre las diferentes clases de errores (falsos positivos y falsos negativos).\n",
    "\n",
    "#### Recall (Sensibilidad o Tasa de Verdaderos Positivos)\n",
    "**Definición:**\n",
    "El recall mide la capacidad del modelo para identificar todas las instancias positivas. Es especialmente importante en contextos donde no detectar una instancia positiva tiene un alto costo.\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP + FN}}\n",
    "$$\n",
    "\n",
    "**Resultado: 0.99**\n",
    "- **Interpretación:** El modelo identifica correctamente el 99% de las instancias positivas. Este alto valor de recall es crucial en situaciones como el diagnóstico médico, donde es importante no pasar por alto ningún caso positivo.\n",
    "\n",
    "#### Precision (Precisión o Valor Predictivo Positivo)\n",
    "**Definición:**\n",
    "La precisión mide la exactitud de las predicciones positivas del modelo. Indica la proporción de verdaderos positivos sobre todas las predicciones positivas.\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}}\n",
    "$$\n",
    "\n",
    "**Resultado: 0.96**\n",
    "- **Interpretación:** El 96% de las instancias que el modelo predice como positivas son realmente positivas. Una alta precisión es importante en casos donde los falsos positivos son costosos.\n",
    "\n",
    "#### F1 Score\n",
    "**Definición:**\n",
    "El F1 Score es la media armónica de la precisión y el recall. Proporciona una única métrica que equilibra la precisión y el recall, especialmente útil cuando se necesita un balance entre ambas.\n",
    "\n",
    "$$\n",
    "\\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision + Recall}}\n",
    "$$\n",
    "\n",
    "**Resultado: 0.97**\n",
    "- **Interpretación:** El F1 Score de 0.97 indica un equilibrio casi perfecto entre precisión y recall, mostrando que el modelo es muy bueno tanto en identificar instancias positivas como en evitar falsos positivos.\n",
    "\n",
    "#### ROC AUC Score (Área bajo la curva ROC)\n",
    "**Definición:**\n",
    "El ROC AUC Score mide la capacidad del modelo para distinguir entre clases. La curva ROC traza la tasa de verdaderos positivos frente a la tasa de falsos positivos en varios umbrales de clasificación.\n",
    "\n",
    "$$\n",
    "\\text{AUC} = \\int_{\\text{ROC}} \\text{d(ROC)}\n",
    "$$\n",
    "\n",
    "**Resultado: 0.96**\n",
    "- **Interpretación:** Un ROC AUC Score de 0.96 significa que el modelo tiene una excelente capacidad para distinguir entre clases positivas y negativas. Cuanto más cercano a 1, mejor es la discriminación.\n",
    "\n",
    "### Conclusión\n",
    "Las métricas obtenidas reflejan un modelo que no solo es preciso en sus predicciones generales (accuracy), sino que también es extremadamente eficaz en identificar correctamente las instancias positivas (recall) y en asegurar que las predicciones positivas sean verdaderamente positivas (precision). El F1 Score y el ROC AUC Score complementan esta evaluación al mostrar un excelente balance y capacidad de discriminación, respectivamente. Estos resultados indican que el modelo está bien ajustado y es robusto para su implementación en un entorno de producción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6df62f-6ad1-43b4-a63e-5c4d86a54f94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
